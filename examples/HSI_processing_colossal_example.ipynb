{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HSI_processing_colossal_example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhrbvVEh2iJd"
      },
      "source": [
        "## Train an classifier for hyperspectral image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrnEMlZg2Ahf"
      },
      "source": [
        "This example is developed in Google Colab. Connect Google Cloud Drive to your Colab to load the dataset later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQfRpkswpqrZ",
        "outputId": "1311fe43-7814-4b70-b464-a26530e1d722"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/My Drive\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab Notebooks',\n",
              " 'Indian_pines_gt.mat',\n",
              " 'Indian_pines_corrected.mat',\n",
              " '1204',\n",
              " 'data']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKR5bcjy3Ja4"
      },
      "source": [
        "Install the enviroment of ColossalAI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP7LvCpG23a2",
        "outputId": "bb8c505c-8538-460e-89ed-1196a9c76690"
      },
      "source": [
        "!pip install ColossalAI deepspeed"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ColossalAI\n",
            "  Downloading colossalai-0.0.1b0-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting deepspeed\n",
            "  Downloading deepspeed-0.5.8.tar.gz (517 kB)\n",
            "\u001b[K     |████████████████████████████████| 517 kB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ColossalAI) (21.3)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.7/dist-packages (from ColossalAI) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ColossalAI) (4.62.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ColossalAI) (5.4.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ColossalAI) (1.19.5)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.7/dist-packages (from ColossalAI) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8->ColossalAI) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.9->ColossalAI) (7.1.2)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 42.5 MB/s \n",
            "\u001b[?25hCollecting hjson\n",
            "  Downloading hjson-3.0.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting triton\n",
            "  Downloading triton-1.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.2 MB 235 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ColossalAI) (3.0.6)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->ColossalAI) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->ColossalAI) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from triton->deepspeed) (3.4.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.5.8-py3-none-any.whl size=532154 sha256=61415a9c5a0f031ab305c7b0f80be29f217cce3dfc8647619621cd98cca5aff8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c6/82/cabd9a300c582a221591fd2c8c997e1f03f601e748aad44e4e\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: triton, tensorboardX, ninja, hjson, deepspeed, ColossalAI\n",
            "Successfully installed ColossalAI-0.0.1b0 deepspeed-0.5.8 hjson-3.0.2 ninja-1.10.2.3 tensorboardX-2.4.1 triton-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVKEurtS4SFS",
        "outputId": "5637acea-2fb3-4a1f-9129-f7ac98c1ccb1"
      },
      "source": [
        "import colossalai\n",
        "from colossalai.engine import Engine, NoPipelineSchedule\n",
        "from colossalai.trainer import Trainer\n",
        "from colossalai.context import Config\n",
        "import torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colossalai should be built with cuda extension to use the FP16 optimizer\n",
            "Colossalai should be built with cuda extension to use the FP16 optimizer\n",
            "apex is required for mixed precision training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpFfhNBD7NSn"
      },
      "source": [
        "Initialize distributed environment for compatibility (we just set the number of parallel processes to 1 for single GPU.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yF7Lc-K7NAS",
        "outputId": "bc12d01f-a9c7-4ac0-fb40-f3f1ae746472"
      },
      "source": [
        "parallel_cfg = Config(dict(parallel=dict(\n",
        "    data=dict(size=1),\n",
        "    pipeline=dict(size=1),\n",
        "    tensor=dict(size=1, mode=None),\n",
        ")))\n",
        "colossalai.init_dist(config=parallel_cfg,\n",
        "          local_rank=0,\n",
        "          world_size=1,\n",
        "          host='127.0.0.1',\n",
        "          port=8888,\n",
        "          backend='nccl')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "colossalai - torch.distributed.distributed_c10d - 2021-12-04 10:10:36,754 INFO: Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "colossalai - torch.distributed.distributed_c10d - 2021-12-04 10:10:36,756 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
            "colossalai - torch.distributed.distributed_c10d - 2021-12-04 10:10:36,764 INFO: Added key: store_based_barrier_key:2 to store for rank: 0\n",
            "colossalai - torch.distributed.distributed_c10d - 2021-12-04 10:10:36,769 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
            "colossalai - torch.distributed.distributed_c10d - 2021-12-04 10:10:36,773 INFO: Added key: store_based_barrier_key:3 to store for rank: 0\n",
            "colossalai - torch.distributed.distributed_c10d - 2021-12-04 10:10:36,775 INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "process rank 0 is bound to device 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppjmMxc_81TK"
      },
      "source": [
        "Download the Hyperspectral image dataset named IndianPines with 17 classes and its groundtruth in 'http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat' and 'http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat', respectively. Put them in your root directory of Google Cloud Drive and load them.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORaHGnLi0is_"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn import preprocessing\n",
        "from scipy import io, misc\n",
        "\n",
        "# Load the image\n",
        "folder_Pine = './'\n",
        "img = io.loadmat(folder_Pine + 'Indian_pines_corrected.mat')\n",
        "img = img['indian_pines_corrected']\n",
        "gt = io.loadmat(folder_Pine + 'Indian_pines_gt.mat')['indian_pines_gt']\n",
        "LABEL_VALUES = [\"Undefined\", \"Alfalfa\", \"Corn-notill\", \"Corn-mintill\",\n",
        "                        \"Corn\", \"Grass-pasture\", \"Grass-trees\",\n",
        "                        \"Grass-pasture-mowed\", \"Hay-windrowed\", \"Oats\",\n",
        "                        \"Soybean-notill\", \"Soybean-mintill\", \"Soybean-clean\",\n",
        "                        \"Wheat\", \"Woods\", \"Buildings-Grass-Trees-Drives\",\n",
        "                        \"Stone-Steel-Towers\"]\n",
        "IGNORED_LABELS = [0]\n",
        "nan_mask = np.isnan(img.sum(axis=-1))\n",
        "if np.count_nonzero(nan_mask) > 0:\n",
        "  print(\"Warning: NaN have been found in the data. It is preferable to remove them beforehand. Learning on NaN data is disabled.\")\n",
        "img[nan_mask] = 0\n",
        "gt[nan_mask] = 0\n",
        "IGNORED_LABELS.append(0)\n",
        "IGNORED_LABELS = list(set(IGNORED_LABELS))\n",
        "# Normalization\n",
        "img = np.asarray(img, dtype='float32')\n",
        "#img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
        "data = img.reshape(np.prod(img.shape[:2]), np.prod(img.shape[2:]))\n",
        "#data = preprocessing.scale(data)\n",
        "data  = preprocessing.minmax_scale(data)\n",
        "img = data.reshape(img.shape)\n",
        "\n",
        "# N_CLASSES = len(LABEL_VALUES) -  len(IGNORED_LABELS)\n",
        "N_CLASSES = len(LABEL_VALUES)\n",
        "# Number of bands (last dimension of the image tensor)\n",
        "N_BANDS = img.shape[-1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IGmGIwm5TfK"
      },
      "source": [
        "Define the generic class named HyperX for a hyperspectral scene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWDx7JIM8VgM"
      },
      "source": [
        "class HyperX(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, gt, **hyperparams):\n",
        "        super(HyperX, self).__init__()\n",
        "        self.data = data\n",
        "        self.label = gt\n",
        "        self.name = hyperparams['dataset']\n",
        "        self.patch_size = hyperparams['patch_size']\n",
        "        self.ignored_labels = set(hyperparams['ignored_labels'])\n",
        "        self.flip_augmentation = hyperparams['flip_augmentation']\n",
        "        self.radiation_augmentation = hyperparams['radiation_augmentation'] \n",
        "        self.mixture_augmentation = hyperparams['mixture_augmentation'] \n",
        "        self.center_pixel = hyperparams['center_pixel']\n",
        "        supervision = hyperparams['supervision']\n",
        "        # Fully supervised : use all pixels with label not ignored\n",
        "        if supervision == 'full':\n",
        "            mask = np.ones_like(gt)\n",
        "            for l in self.ignored_labels:\n",
        "                mask[gt == l] = 0\n",
        "        # Semi-supervised : use all pixels, except padding\n",
        "        elif supervision == 'semi':\n",
        "            mask = np.ones_like(gt)\n",
        "        x_pos, y_pos = np.nonzero(mask)\n",
        "        p = self.patch_size // 2\n",
        "        self.indices = np.array([(x,y) for x,y in zip(x_pos, y_pos) if x > p and x < data.shape[0] - p and y > p and y < data.shape[1] - p])\n",
        "        self.labels = [self.label[x,y] for x,y in self.indices]\n",
        "        np.random.shuffle(self.indices)\n",
        "    @staticmethod\n",
        "    def flip(*arrays):\n",
        "        horizontal = np.random.random() > 0.5\n",
        "        vertical = np.random.random() > 0.5\n",
        "        if horizontal:\n",
        "            arrays = [np.fliplr(arr) for arr in arrays]\n",
        "        if vertical:\n",
        "            arrays = [np.flipud(arr) for arr in arrays]\n",
        "        return arrays\n",
        "    @staticmethod\n",
        "    def radiation_noise(data, alpha_range=(0.9, 1.1), beta=1/25):\n",
        "        alpha = np.random.uniform(*alpha_range)\n",
        "        noise = np.random.normal(loc=0., scale=1.0, size=data.shape)\n",
        "        return alpha * data + beta * noise\n",
        "    def mixture_noise(self, data, label, beta=1/25):\n",
        "        alpha1, alpha2 = np.random.uniform(0.01, 1., size=2)\n",
        "        noise = np.random.normal(loc=0., scale=1.0, size=data.shape)\n",
        "        data2 = np.zeros_like(data)\n",
        "        for  idx, value in np.ndenumerate(label):\n",
        "            if value not in self.ignored_labels:\n",
        "                l_indices = np.nonzero(self.labels == value)[0]\n",
        "                l_indice = np.random.choice(l_indices)\n",
        "                assert(self.labels[l_indice] == value)\n",
        "                x, y = self.indices[l_indice]\n",
        "                data2[idx] = self.data[x,y]\n",
        "        return (alpha1 * data + alpha2 * data2) / (alpha1 + alpha2) + beta * noise\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    def __getitem__(self, i):\n",
        "        x, y = self.indices[i]\n",
        "        x1, y1 = x - self.patch_size // 2, y - self.patch_size // 2\n",
        "        x2, y2 = x1 + self.patch_size, y1 + self.patch_size\n",
        "        data = self.data[x1:x2, y1:y2]\n",
        "        label = self.label[x1:x2, y1:y2]\n",
        "        if self.flip_augmentation and self.patch_size > 1:\n",
        "            # Perform data augmentation (only on 2D patches)\n",
        "            data, label = self.flip(data, label)\n",
        "        if self.radiation_augmentation and np.random.random() < 0.1:\n",
        "                data = self.radiation_noise(data)\n",
        "        if self.mixture_augmentation and np.random.random() < 0.2:\n",
        "                data = self.mixture_noise(data, label)\n",
        "        # Copy the data into numpy arrays (PyTorch doesn't like numpy views)\n",
        "        data = np.asarray(np.copy(data), dtype='float32')\n",
        "        label = np.asarray(np.copy(label), dtype='int64')\n",
        "        data = np.expand_dims(data, axis=0)\n",
        "        # Load the data into PyTorch tensors\n",
        "        data = torch.from_numpy(data)\n",
        "        label = torch.from_numpy(label)\n",
        "        targets = label.view(1)\n",
        "\n",
        "        return data, targets\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-it2VZR7P3r"
      },
      "source": [
        "Define the train_loader and test_loader for traning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s7WXDG55rqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d838425d-43b9-4457-8aef-6e458866cdb2"
      },
      "source": [
        "import sklearn.model_selection\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "hyperparams = {'batch_size': 1, \n",
        "        'dataset': 'IndianPines',\n",
        "        'patch_size': 1,\n",
        "        'ignored_labels': [0],\n",
        "        'flip_augmentation': False,\n",
        "        'radiation_augmentation': False, \n",
        "        'mixture_augmentation': False,\n",
        "        'center_pixel': True,\n",
        "        'supervision': 'full'\n",
        "        }\n",
        "\n",
        "def sample_gt(gt, train_size, mode='random'):\n",
        "\n",
        "    indices = np.nonzero(gt)\n",
        "    X = list(zip(*indices)) \n",
        "    y1 = gt[indices]\n",
        "    y = y1.ravel() \n",
        "    train_gt = np.zeros_like(gt) \n",
        "    test_gt = np.zeros_like(gt)\n",
        "    if train_size > 1:\n",
        "       train_size = int(train_size)\n",
        "    if mode == 'random':\n",
        "       train_indices, test_indices = sklearn.model_selection.train_test_split(X, train_size=train_size, stratify=y)\n",
        "       train_indices = [list(t) for t in zip(*train_indices)]\n",
        "       test_indices = [list(t) for t in zip(*test_indices)]\n",
        "       train_gt[train_indices] = gt[train_indices]\n",
        "       test_gt[test_indices] = gt[test_indices]\n",
        "    return train_gt, test_gt\n",
        "train_gt, test_gt = sample_gt(gt, 0.1, mode='random')\n",
        "# Generate the dataset\n",
        "train_dataset = HyperX(img, train_gt, **hyperparams)\n",
        "test_dataset = HyperX(img, test_gt, **hyperparams)    \n",
        "print(\"HSI train dataset\")\n",
        "print(train_dataset)\n",
        "train_loader = data.DataLoader(train_dataset,\n",
        "                batch_size=hyperparams['batch_size'],\n",
        "                shuffle=True)\n",
        "test_loader = data.DataLoader(test_dataset,\n",
        "               batch_size=hyperparams['batch_size'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HSI train dataset\n",
            "<__main__.HyperX object at 0x7fa7771ed810>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvPbfLLR9NzC"
      },
      "source": [
        "Define a simple NN-based network here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ_y7lBG09LS"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "class HSI_processing(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple NN-based network\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            init.kaiming_normal_(m.weight)\n",
        "            init.zeros_(m.bias)\n",
        "\n",
        "    def __init__(self, input_channels=200, n_classes=17, dropout=False):\n",
        "        super(HSI_processing, self).__init__()\n",
        "        self.use_dropout = dropout\n",
        "        if dropout:\n",
        "            self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc1 = nn.Linear(input_channels, 2048)\n",
        "        self.fc2 = nn.Linear(2048, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 2048)\n",
        "        self.fc4 = nn.Linear(2048, n_classes)\n",
        "        self.apply(self.weight_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1) #(200,1)\n",
        "        x = F.relu(self.fc1(x))   #(1,2048)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x  #(1,17)\n",
        "\n",
        "model = HSI_processing().cuda()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgsszAmM9dYZ"
      },
      "source": [
        "Define a Loss function and optimizer to initialize `Engine` and `Trainer`. Use the hook to compute and print loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtaDoCax1BCf",
        "outputId": "4a5af21b-cd19-4963-f8b0-2916fffe2a3d"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "schedule = NoPipelineSchedule()\n",
        "\n",
        "engine = Engine(\n",
        "        model=model,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        lr_scheduler=None,\n",
        "        schedule=schedule\n",
        "    )\n",
        "trainer = Trainer(engine=engine,\n",
        "          hooks_cfg=[dict(type='LossHook'), dict(type='LogMetricByEpochHook'), dict(type='AccuracyHook')],\n",
        "          verbose=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "colossalai - rank_0 - 2021-12-04 10:11:48,617 WARNING: No gradient handler is set up, please make sure you do not need to all-reduce the gradients after a training step.\n",
            "colossalai - rank_0 - 2021-12-04 10:11:48,626 INFO: build LogMetricByEpochHook for train, priority = 1\n",
            "colossalai - rank_0 - 2021-12-04 10:11:48,628 INFO: build LossHook for train, priority = 10\n",
            "colossalai - rank_0 - 2021-12-04 10:11:48,630 INFO: build AccuracyHook for train, priority = 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JR2TuvH99Ik"
      },
      "source": [
        "Train model for 10 epochs and it will be evaluated every 3 epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-J3IP-J1sfx",
        "outputId": "7605bddf-e84d-4432-9ea8-c1f52d29d2e9"
      },
      "source": [
        "num_epochs = 10\n",
        "test_interval = 1\n",
        "trainer.fit(\n",
        "        train_dataloader=train_loader,\n",
        "        test_dataloader=test_loader,\n",
        "        max_epochs=num_epochs,\n",
        "        display_progress=True,\n",
        "        test_interval=test_interval\n",
        "    )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 0 train]: 100%|██████████| 1020/1020 [00:13<00:00, 77.13it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:12:04,413 INFO: Training - Epoch 1 - LogMetricByEpochHook: Loss = 1.81093\n",
            "[Epoch 0 val]: 100%|██████████| 9156/9156 [00:12<00:00, 747.87it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:12:16,674 INFO: Testing - Epoch 1 - LogMetricByEpochHook: Loss = 1.42704, Accuracy = 0.48012\n",
            "[Epoch 1 train]: 100%|██████████| 1020/1020 [00:13<00:00, 76.09it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:12:30,103 INFO: Training - Epoch 2 - LogMetricByEpochHook: Loss = 1.43416\n",
            "[Epoch 1 val]: 100%|██████████| 9156/9156 [00:11<00:00, 801.98it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:12:41,528 INFO: Testing - Epoch 2 - LogMetricByEpochHook: Loss = 1.40988, Accuracy = 0.49792\n",
            "[Epoch 2 train]: 100%|██████████| 1020/1020 [00:13<00:00, 76.19it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:12:54,933 INFO: Training - Epoch 3 - LogMetricByEpochHook: Loss = 1.31126\n",
            "[Epoch 2 val]: 100%|██████████| 9156/9156 [00:12<00:00, 757.60it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:13:07,026 INFO: Testing - Epoch 3 - LogMetricByEpochHook: Loss = 1.24297, Accuracy = 0.55898\n",
            "[Epoch 3 train]: 100%|██████████| 1020/1020 [00:13<00:00, 75.68it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:13:20,524 INFO: Training - Epoch 4 - LogMetricByEpochHook: Loss = 1.21081\n",
            "[Epoch 3 val]: 100%|██████████| 9156/9156 [00:11<00:00, 801.15it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:13:31,963 INFO: Testing - Epoch 4 - LogMetricByEpochHook: Loss = 1.08171, Accuracy = 0.64275\n",
            "[Epoch 4 train]: 100%|██████████| 1020/1020 [00:13<00:00, 75.93it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:13:45,416 INFO: Training - Epoch 5 - LogMetricByEpochHook: Loss = 1.09705\n",
            "[Epoch 4 val]: 100%|██████████| 9156/9156 [00:11<00:00, 771.13it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:13:57,299 INFO: Testing - Epoch 5 - LogMetricByEpochHook: Loss = 1.06161, Accuracy = 0.58563\n",
            "[Epoch 5 train]: 100%|██████████| 1020/1020 [00:13<00:00, 75.47it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:14:10,836 INFO: Training - Epoch 6 - LogMetricByEpochHook: Loss = 1.02669\n",
            "[Epoch 5 val]: 100%|██████████| 9156/9156 [00:11<00:00, 794.66it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:14:22,369 INFO: Testing - Epoch 6 - LogMetricByEpochHook: Loss = 1.01064, Accuracy = 0.65203\n",
            "[Epoch 6 train]: 100%|██████████| 1020/1020 [00:13<00:00, 75.89it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:14:35,829 INFO: Training - Epoch 7 - LogMetricByEpochHook: Loss = 0.95567\n",
            "[Epoch 6 val]: 100%|██████████| 9156/9156 [00:11<00:00, 781.32it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:14:47,556 INFO: Testing - Epoch 7 - LogMetricByEpochHook: Loss = 1.06380, Accuracy = 0.56979\n",
            "[Epoch 7 train]: 100%|██████████| 1020/1020 [00:13<00:00, 75.52it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:15:01,082 INFO: Training - Epoch 8 - LogMetricByEpochHook: Loss = 0.90912\n",
            "[Epoch 7 val]: 100%|██████████| 9156/9156 [00:11<00:00, 793.38it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:15:12,632 INFO: Testing - Epoch 8 - LogMetricByEpochHook: Loss = 0.88896, Accuracy = 0.67267\n",
            "[Epoch 8 train]: 100%|██████████| 1020/1020 [00:13<00:00, 75.76it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:15:26,115 INFO: Training - Epoch 9 - LogMetricByEpochHook: Loss = 0.87150\n",
            "[Epoch 8 val]: 100%|██████████| 9156/9156 [00:11<00:00, 781.29it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:15:37,844 INFO: Testing - Epoch 9 - LogMetricByEpochHook: Loss = 0.90919, Accuracy = 0.67366\n",
            "[Epoch 9 train]: 100%|██████████| 1020/1020 [00:13<00:00, 75.34it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:15:51,402 INFO: Training - Epoch 10 - LogMetricByEpochHook: Loss = 0.81109\n",
            "[Epoch 9 val]: 100%|██████████| 9156/9156 [00:11<00:00, 814.01it/s]\n",
            "colossalai - rank_0 - 2021-12-04 10:16:02,660 INFO: Testing - Epoch 10 - LogMetricByEpochHook: Loss = 0.77782, Accuracy = 0.71745\n"
          ]
        }
      ]
    }
  ]
}
